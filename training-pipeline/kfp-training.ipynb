{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex AI Pipeline(Kubeflow) Training model pipeline\n",
    "\n",
    "This sample kubeflow pipeline is for training the simple regression model.\n",
    "</br>Newyork taxi data used. </br>\n",
    "https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page </br>\n",
    "\n",
    "This pipeline generate the model to predict the driving duration, \n",
    "based on pickup location (PULocationID), dropoff location(DOLocationID) and trip distance.\n",
    "\n",
    "Generated model will be registered to Model Registry in Vertex AI,\n",
    "This model will be deployed to Online Prediction in Vertex AI.\n",
    "\n",
    "For scheduing test purpose, I used just random month from 2022 to see the whole process works fine\n",
    "running every minute.\n",
    "\n",
    "This Kubeflow training pipeline consist of steps following.\n",
    "\n",
    "\n",
    "1. Download the data\n",
    "2. Preprocessing\n",
    "3. Training\n",
    "4. Evaluating\n",
    "5. Register the model\n",
    "6. Deploy the model (Depending on the evaluation result)\n",
    "7. Get test prediction results\n",
    "\n",
    "![img_info](./kubeflow-graph.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install {USER_FLAG} google-cloud-aiplatform==1.0.0 --upgrade\n",
    "!pip3 install {USER_FLAG} kfp google-cloud-pipeline-components==0.1.1 --upgrade\n",
    "!pip install -U google-cloud-aiplatform \"shapely<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  yorrr78-dev-111111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://yorrr78-dev-111111-mlops-bucket/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'yorrr78-dev-111111-mlops-bucket' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME=\"gs://\" + PROJECT_ID + \"-mlops-bucket\"\n",
    "!gsutil mb -l asia $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from google.cloud import storage\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-Learn version: 1.4.0\n",
      "Python 3.10.13\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import sklearn; print('Scikit-Learn version: {}'.format(sklearn.__version__))\"\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://yorrr78-dev-111111-mlops-bucket/pipeline_root/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"asia-northeast3\"\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Download the data\n",
    "@component(\n",
    "    base_image=\"python:3.10-slim\", \n",
    "    output_component_file=\"./components/ingestion-component.yaml\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"tqdm\"]\n",
    ")\n",
    "def download_taxi_data(\n",
    "    bucket_name:str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Data will be downloaded from 'https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page'\n",
    "    which is public dataset from New York.\n",
    "    This downloaded parquet file will be located in 'data'folder of your GCS bucket.\n",
    "    \"\"\"\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    from tqdm import tqdm\n",
    "    import requests\n",
    "    import os\n",
    "    import logging\n",
    "    from datetime import datetime, timedelta\n",
    "    import random \n",
    "    \n",
    "    \n",
    "    def get_file_name_by_date():\n",
    "        \"\"\"Get the file name from pipeline running date\"\"\"\n",
    "        \n",
    "        # # Get the previous month data file name \n",
    "        # current_time = datetime.now() - timedelta(days=31)\n",
    "        # file_month = '%02d' % current_time.month\n",
    "        # file = f'green_tripdata_2022-{file_month}.parquet'\n",
    "\n",
    "        # Test purpose\n",
    "        random_number = random.randint(1, 12)\n",
    "        current_month = '%02d' % random_number\n",
    "\n",
    "        return f'green_tripdata_2022-{current_month}.parquet'\n",
    "    \n",
    "    \n",
    "    def download_to_local(download_url, file):\n",
    "        \"\"\"Download the target file from internet to local\"\"\"\n",
    "        \n",
    "        if os.path.isfile(f'./data/{file}'):\n",
    "            logging.info('Already exist')\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            if not os.path.exists('data'):\n",
    "                os.mkdir('data')\n",
    "            \n",
    "            file_url = download_url + file\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            logging.info(f'downloading.. {file}')\n",
    "            \n",
    "            with open(f'./data/{file}', 'wb') as f_in:\n",
    "                for chunk in tqdm(response.iter_content()):\n",
    "                    f_in.write(chunk)\n",
    "            logging.info('Download finished!')\n",
    "\n",
    "    \n",
    "    def upload_to_blob(bucket_name, file):\n",
    "        \"\"\"Upload the temporary file to the GCS blob\"\"\"\n",
    "        \n",
    "        with open(f'./data/{file}', 'rb') as f_out:\n",
    "            result = blob.upload_from_file(f_out)\n",
    "\n",
    "        logging.info(f'Finished downloading {file} to GCS bucket {bucket_name}')\n",
    "    \n",
    "    \n",
    "    download_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/'\n",
    "    file = get_file_name_by_date()\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(f'data/{file}')\n",
    "    \n",
    "    if blob.exists():\n",
    "        logging.info(f'Blob {file} already exists in bucket {bucket_name}')\n",
    "        return f'gs://{bucket_name}/data/{file}'\n",
    "    \n",
    "    download_to_local(download_url, file)\n",
    "    upload_to_blob(bucket_name, file)\n",
    "    \n",
    "    return f'gs://{bucket_name}/data/{file}'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Preprocess data\n",
    "@component(\n",
    "    base_image=\"python:3.10-slim\", \n",
    "    output_component_file=\"./components/preprocessing-component.yaml\",\n",
    "    packages_to_install=[\"pandas\", \"tqdm\", \"fastparquet\", \"pyarrow\", \"numpy\"]\n",
    ")\n",
    "def preprocess_taxi_data(\n",
    "    input_data: str,\n",
    "    train_size: float,\n",
    "    valid_size: float,\n",
    "    test_size: float,\n",
    "    train_data: Output[Dataset],\n",
    "    valid_data: Output[Dataset],\n",
    "    test_data: Output[Dataset],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Data will be preprocessed this step.\n",
    "    - Model type: regression model\n",
    "    - Target column: duration (will be calculated with pickup time & dropoff time) \n",
    "    - Feature columns: pickup location (PULocationID), dropoff location(DOLocationID) and trip distance\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    df = pd.read_parquet(input_data)\n",
    "    columns = [\n",
    "        'PULocationID',\n",
    "        'DOLocationID',\n",
    "        'lpep_dropoff_datetime',\n",
    "        'lpep_pickup_datetime',\n",
    "        'trip_distance',\n",
    "    ]\n",
    "    df = df[columns]\n",
    "\n",
    "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration = df.duration.dt.total_seconds() / 60\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)] # Get rid of outliers\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    train_ds, valid_ds, test_ds = np.split(df.sample(frac=1, random_state=42), [int((train_size)*len(df)), int((1-test_size)*len(df))])\n",
    "    \n",
    "    # Use .path for passing data in Artifact\n",
    "    train_ds.to_parquet(train_data.path, index=False) \n",
    "    valid_ds.to_parquet(valid_data.path, index=False)\n",
    "    test_ds.to_parquet(test_data.path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3 Train data\n",
    "@component(\n",
    "    base_image=\"python:3.9-slim\", \n",
    "    output_component_file=\"./components/training-component.yaml\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"fastparquet\", \"pyarrow\"]\n",
    ")\n",
    "def train_taxi_data(\n",
    "    train_data: Input[Dataset], \n",
    "    params: dict,\n",
    "    model: Output[Model],\n",
    ")-> str:\n",
    "    \"\"\"\n",
    "    This step will train the regression model using Scikit-learn randomforest regressor.\n",
    "    - Model type: regression model\n",
    "    - Target column: duration (will be calculated with pickup time & dropoff time) \n",
    "    - Feature columns: pickup location (PULocationID), dropoff location(DOLocationID) and trip distance\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    import sklearn\n",
    "    \n",
    "    \n",
    "    def prepare_dictionaries(df: pd.DataFrame):\n",
    "        \"\"\"Composite\"\"\"\n",
    "        df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "        categorical = ['PU_DO']\n",
    "        numerical = ['trip_distance']\n",
    "        dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "        return dicts\n",
    "\n",
    "    \n",
    "    def train_model(X_train, y_train):\n",
    "        \"\"\"Model training\"\"\"\n",
    "        pipeline = make_pipeline(\n",
    "            DictVectorizer(),\n",
    "            RandomForestRegressor(**params, n_jobs=-1)\n",
    "        )\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        return pipeline\n",
    "        \n",
    "    \n",
    "    df = pd.read_parquet(train_data.path)\n",
    "    target = 'duration'\n",
    "    X_train = prepare_dictionaries(df)\n",
    "    y_train = df[target].values\n",
    "    \n",
    "    my_model = train_model(X_train, y_train)\n",
    "    \n",
    "    model.metadata[\"model_name\"] = \"RandomForestRegressor\"\n",
    "    model.metadata[\"framework\"] = \"sklearn\"\n",
    "    model.metadata[\"framework_version\"] = sklearn.__version__\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    \n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(my_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate model\n",
    "@component(\n",
    "    base_image=\"python:3.10-slim\", \n",
    "    output_component_file=\"./components/evaluating-component.yaml\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"tqdm\", \"fastparquet\", \"pyarrow\", \"numpy\"]\n",
    ")\n",
    "def evaluate_model(\n",
    "    val_data: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    target_column_name: str,\n",
    "    deployment_metric: str,\n",
    "    deployment_metric_threshold: float,\n",
    "    kpi: Output[Metrics],\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\", [\n",
    "        (\"dep_decision\", str),\n",
    "    ]\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluating model good enough to deploy.\n",
    "    \n",
    "    Data from preprocessing step will be used for evalutaion (valid_data).\n",
    "    given 'deployment_metric', 'deployment_metric_threshold' values are compared.\n",
    "    if passing threshold, model will be registered aand deployed,\n",
    "    if not, this is the pipeline end.\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from tqdm import tqdm\n",
    "    import requests\n",
    "    import os\n",
    "    import pickle\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    def prepare_dictionaries(df: pd.DataFrame):\n",
    "        \"\"\"Composite\"\"\"\n",
    "        df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "        categorical = ['PU_DO']\n",
    "        numerical = ['trip_distance']\n",
    "        dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "        return dicts\n",
    "    \n",
    "    \n",
    "    val_ds = pd.read_parquet(val_data.path)\n",
    "    target = target_column_name\n",
    "    \n",
    "    X_test = prepare_dictionaries(val_ds)\n",
    "    y_test = val_ds[target].values\n",
    "    \n",
    "    logging.info(f\"model.path : {model.path}\")\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    logging.info(f\"file_name : {file_name}\")\n",
    "    \n",
    "    with open(file_name, 'rb') as f:  \n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "    mae = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
    "    mse = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true=y_test, y_pred=y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    model_metrics = {\n",
    "        \"r2\": r2, \n",
    "        \"mae\": mae, \n",
    "        \"mape\": mape, \n",
    "        \"mse\" : mse, \n",
    "        \"rmse\" : rmse,\n",
    "    }\n",
    "\n",
    "    logging.info(f\"Adjusted_R2 : {r2}\")\n",
    "    logging.info(f\"Mean Absolute Error : {mae}\")\n",
    "    logging.info(f\"Mean Absolute Percentage Error : {round(mape,4)*100}%\")\n",
    "    logging.info(f\"Mean Squared Error : {mse}\")\n",
    "    logging.info(f\"Root Mean Squared Error : {rmse}\")\n",
    "    \n",
    "    kpi.log_metric(\"Adjusted_R2\", float(r2))\n",
    "    kpi.log_metric(\"Mean Absolute Error\", float(mae))\n",
    "    kpi.log_metric(\"Mean Absolute Percentage Error\", float(mape))\n",
    "    kpi.log_metric(\"Mean Squared Error\", float(mse))\n",
    "    kpi.log_metric(\"Root Mean Squared Error\", float(rmse))\n",
    "    \n",
    "    actual_metric_value = model_metrics.get(deployment_metric)\n",
    "    \n",
    "    if actual_metric_value >= deployment_metric_threshold:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "        \n",
    "    return (dep_decision,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#5 Register model\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn\" , \"fsspec\" , \"gcsfs\" , \"google-cloud-aiplatform\"], \n",
    "    base_image=\"python:3.9-slim\", \n",
    "    output_component_file=\"./components/registering-component.yaml\"\n",
    ")\n",
    "def register_model(\n",
    "    serving_container_uri: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    model_name: str, \n",
    "    model: Input[Model], \n",
    ")-> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"model_resource_name\", str),  # Return parameter.\n",
    "    ],\n",
    "):\n",
    "    \"\"\"Regster the model to Vertex AI model registry\"\"\"\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    import logging\n",
    "    \n",
    "    \n",
    "    logging.info(f\"serving_container_uri: {serving_container_uri}\")\n",
    "    logging.info(f\"project_id: {project_id}\")\n",
    "    logging.info(f\"region: {region}\")\n",
    "    logging.info(f\"model: {model}\")\n",
    "    logging.info(f\"model.uri: {model.uri[:-5]}\")\n",
    "    \n",
    "    # for artifact_uri arg,\n",
    "    # The model name must be one of: saved_model.pb, model.pkl, model.joblib, or model.bst, depending on which library you used.\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name= model_name,\n",
    "        # artifact_uri=model.uri,\n",
    "        artifact_uri=model.uri[:-5],\n",
    "        serving_container_image_uri=serving_container_uri\n",
    "    )\n",
    "    \n",
    "    return (model.resource_name,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#6 Deploy model\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\",  \"scikit-learn\" , \"fsspec\" , \"gcsfs\", \"google-cloud-aiplatform\"], \n",
    "    base_image=\"python:3.9-slim\", \n",
    "    output_component_file=\"./components/deploying-component.yaml\"\n",
    ")\n",
    "def deploy_model(\n",
    "    model_resource_name: str ,\n",
    "    project_id: str ,\n",
    "    region: str\n",
    ")-> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"endpoint_resource_name\", str),\n",
    "    ],\n",
    "):\n",
    "    \"\"\"Deploy the model to Vertex AI for online prediction\"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "    import logging\n",
    "    \n",
    "    \n",
    "    logging.info(f\"model_resource_name : {model_resource_name}\")\n",
    "    logging.info(f\"project_id : {project_id}\")\n",
    "    logging.info(f\"region : {region}\")\n",
    "    \n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    \n",
    "    model = aiplatform.Model(model_resource_name)\n",
    "    endpoint = model.deploy(\n",
    "        machine_type=\"n1-standard-2\",\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1\n",
    "    )\n",
    "    \n",
    "    return (endpoint.resource_name,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#7 Test prediction\n",
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"pyarrow\", \"google-cloud-aiplatform\", \"google-cloud-storage\"], \n",
    "    base_image=\"python:3.9-slim\", \n",
    "    output_component_file=\"./components/predicting-component.yaml\"\n",
    ")\n",
    "def test_prediction(\n",
    "    project_id: str ,\n",
    "    region: str,\n",
    "    test_ds: Input[Dataset],\n",
    "    endpoint_resource_name: str,\n",
    "    bucket_name: str,\n",
    "    prediction_blob: str\n",
    ") -> str:\n",
    "    \n",
    "    \"\"\"Get predictions from model served with test data\"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud import storage\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    \n",
    "    logging.info(f\"testds: {test_ds}\")\n",
    "    logging.info(f\"test_ds: {test_ds.path}\")\n",
    "    logging.info(f\"model_resource_name : {region}\")\n",
    "\n",
    "    def prepare_dictionaries(df: pd.DataFrame):\n",
    "        \"\"\"Composite\"\"\"\n",
    "        df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "        categorical = ['PU_DO']\n",
    "        numerical = ['trip_distance']\n",
    "        dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "        return dicts\n",
    "    \n",
    "    \n",
    "    def get_predictions(instances, region, endpoint_resource_name):\n",
    "        \"\"\"Get predictions from deployed model\"\"\"\n",
    "        \n",
    "        client_options = {\"api_endpoint\": f\"{region}-aiplatform.googleapis.com\"}\n",
    "        client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "\n",
    "        response = client.predict(\n",
    "            endpoint=endpoint_resource_name, \n",
    "            instances=instances\n",
    "        )\n",
    "\n",
    "        predictions = {\n",
    "            \"predictions\": list(response.predictions)\n",
    "        }\n",
    "\n",
    "        # Write predictions to a JSON file\n",
    "        output_file = \"predictions.json\"\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(predictions, f)\n",
    "        \n",
    "        return output_file\n",
    "            \n",
    "        \n",
    "    def upload_to_gcs(prediction_file, bucket_name, prediction_blob):\n",
    "        \"\"\"Upload the file to Google Cloud Storage\"\"\"\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(bucket_name)\n",
    "        blob = bucket.blob(prediction_blob)\n",
    "        blob.upload_from_filename(prediction_file)\n",
    "\n",
    "        logging.info(f\"Predictions uploaded to gs://{bucket_name}/{prediction_blob} successfully!\")    \n",
    "        \n",
    "        \n",
    "    test_ds = pd.read_parquet(test_ds.path)\n",
    "    test_dict_ds = prepare_dictionaries(test_ds)\n",
    "    \n",
    "    prediction_file = get_predictions(\n",
    "        instances=test_dict_ds,\n",
    "        region=region,\n",
    "        endpoint_resource_name=endpoint_resource_name,\n",
    "    )\n",
    "    \n",
    "    upload_to_gcs(prediction_file, bucket_name, prediction_blob)\n",
    "    \n",
    "    return f'gs://{bucket_name}/{prediction_blob}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.json\n",
    "{\n",
    "    \"project_id\": \"yorrr78-dev-111111\",\n",
    "    \"region\": \"asia-northeast3\",\n",
    "    \"pipeline_name\": \"taxi-data-model-kfp-test\",\n",
    "    \"pipeline_package_path\": \"nytaxi_model_pipeline_job.json\",\n",
    "    \"bucket_name\": \"yorrr78-dev-111111-mlops-bucket\",\n",
    "    \"train_size\": 0.8,\n",
    "    \"valid_size\": 0.1,\n",
    "    \"test_size\": 0.1,\n",
    "    \"deployment_metric\": \"r2\",\n",
    "    \"deployment_metric_threshold\": 0.7,\n",
    "    \"serving_container_uri\": \"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\",\n",
    "    \"model_name\": \"sklearn-kubeflow-nytaxi-regression-model\",\n",
    "    \"params\": {\n",
    "        \"max_depth\": 20,\n",
    "        \"n_estimators\": 100,\n",
    "        \"min_samples_leaf\": 10,\n",
    "        \"random_state\": 0\n",
    "    },\n",
    "    \"prediction_blob\": \"predictions/predictions.json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kfp_build_training_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kfp_build_training_pipeline.py\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.components import load_component_from_file\n",
    "\n",
    "\n",
    "download_taxi_data = load_component_from_file(\"./components/ingestion-component.yaml\")\n",
    "preprocess_taxi_data = load_component_from_file(\"./components/preprocessing-component.yaml\")\n",
    "train_taxi_data = load_component_from_file(\"./components/training-component.yaml\")\n",
    "evaluate_model = load_component_from_file(\"./components/evaluating-component.yaml\")\n",
    "register_model = load_component_from_file(\"./components/registering-component.yaml\")\n",
    "deploy_model = load_component_from_file(\"./components/deploying-component.yaml\")\n",
    "test_prediction = load_component_from_file(\"./components/predicting-component.yaml\")\n",
    "\n",
    "\n",
    "#read configuration from file\n",
    "with open(\"config.json\") as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "PIPELINE_NAME = config.get(\"pipeline_name\")\n",
    "PACKAGE_PATH = config.get(\"pipeline_package_path\")\n",
    "BUCKET_NAME = config.get(\"bucket_name\") \n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/\"\n",
    "    \n",
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def model_pipeline(\n",
    "    project_id:str=\"\",\n",
    "    region:str=\"asia-northeast3\",\n",
    "    pipeline_name:str=\"\",\n",
    "    pipeline_package_path:str=\"\",\n",
    "    bucket_name:str=\"\",\n",
    "    train_size:float=0.8,\n",
    "    valid_size:float=0.1,\n",
    "    test_size:float=0.1,\n",
    "    deployment_metric:str=\"r2\",\n",
    "    deployment_metric_threshold:float=0.7,\n",
    "    serving_container_uri:str='asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest',\n",
    "    model_name:str='sklearn-kubeflow-nytaxi-regression-model',\n",
    "    params:dict={\n",
    "        'max_depth': 20,\n",
    "        'n_estimators': 100,\n",
    "        'min_samples_leaf': 10,\n",
    "        'random_state': 0\n",
    "    },\n",
    "    prediction_blob:str='predictions/predictions.json',\n",
    "):\n",
    "    ingestion_task = download_taxi_data(bucket_name)\n",
    "    processing_task = preprocess_taxi_data(\n",
    "        input_data=ingestion_task.output,\n",
    "        train_size=train_size,\n",
    "        valid_size=valid_size,\n",
    "        test_size=test_size\n",
    "    )\n",
    "    training_task = train_taxi_data(\n",
    "        train_data=processing_task.outputs[\"train_data\"],\n",
    "        params=params,\n",
    "    )\n",
    "    evaluating_task = evaluate_model(\n",
    "        val_data=processing_task.outputs[\"valid_data\"],\n",
    "        model=training_task.outputs[\"model\"],\n",
    "        target_column_name='duration',\n",
    "        deployment_metric=deployment_metric,\n",
    "        deployment_metric_threshold=deployment_metric_threshold,\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        evaluating_task.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "        # check the container uri list here: https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#scikit-learn\n",
    "        # deploy only if metric value exceeds deployment threshold\n",
    "        registering_task = register_model(\n",
    "            serving_container_uri=serving_container_uri,\n",
    "            project_id=project_id,\n",
    "            region=region,\n",
    "            model_name=model_name,\n",
    "            model=training_task.outputs[\"model\"],\n",
    "        )\n",
    "        \n",
    "        deploying_task = deploy_model(\n",
    "            model_resource_name = registering_task.outputs[\"model_resource_name\"],\n",
    "            project_id=project_id,\n",
    "            region=region\n",
    "        )\n",
    "        \n",
    "        predicting_task = test_prediction(\n",
    "            project_id=project_id,\n",
    "            region=region,\n",
    "            test_ds=processing_task.outputs[\"test_data\"],\n",
    "            endpoint_resource_name=deploying_task.outputs[\"endpoint_resource_name\"],\n",
    "            bucket_name=bucket_name,\n",
    "            prediction_blob=prediction_blob,\n",
    "        )\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=model_pipeline, package_path=PACKAGE_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kfp_run_training_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kfp_run_training_pipeline.py\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import yaml\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "with open(\"config.json\") as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "DISPLAY_NAME = config.get(\"pipeline_name\")\n",
    "PACKAGE_PATH = config.get(\"pipeline_package_path\")\n",
    "BUCKET_NAME = config.get(\"bucket_name\") \n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/\"\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=PACKAGE_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values=config,\n",
    ")\n",
    "\n",
    "# For test purpose, execute pipeline every min\n",
    "job_schedule = job.create_schedule(\n",
    "  display_name=DISPLAY_NAME,\n",
    "  cron=\"* * * * *\",\n",
    "  max_concurrent_run_count=5,\n",
    "  max_run_count=30,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!python3 kfp_build_training_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJobSchedule\n",
      "PipelineJobSchedule created. Resource name: projects/616906371504/locations/us-central1/schedules/3306143503792209920\n",
      "To use this PipelineJobSchedule in another session:\n",
      "schedule = aiplatform.PipelineJobSchedule.get('projects/616906371504/locations/us-central1/schedules/3306143503792209920')\n",
      "View Schedule:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/schedules/3306143503792209920?project=616906371504\n"
     ]
    }
   ],
   "source": [
    "!python3 kfp_run_training_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./config.json [Content-Type=application/json]...\n",
      "/ [1 files][  729.0 B/  729.0 B]                                                \n",
      "Operation completed over 1 objects/729.0 B.                                      \n",
      "Copying file://./nytaxi_model_pipeline_job.json [Content-Type=application/json]...\n",
      "/ [1 files][ 39.7 KiB/ 39.7 KiB]                                                \n",
      "Operation completed over 1 objects/39.7 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp ./config.json $BUCKET_NAME\n",
    "! gsutil cp ./nytaxi_model_pipeline_job.json $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create by api_client.create_run_from_job_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"taxi-data-training\",\n",
    "    description=\"NYC taxi open data for testing VertexAI Kubeflow\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def model_pipeline(\n",
    "    project_id:str='yorrr78-dev-111111',\n",
    "    region:str='asia-northeast3',\n",
    "    bucket_name:str='yorrr78-dev-111111-mlops-bucket',\n",
    "    train_size:float=0.8,\n",
    "    valid_size:float=0.1,\n",
    "    test_size:float=0.1,\n",
    "    deployment_metric:str=\"r2\",\n",
    "    deployment_metric_threshold:float=0.7,\n",
    "    serving_container_uri:str='asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest',\n",
    "    model_name:str='sklearn-kubeflow-nytaxi-regression-model',\n",
    "    params:dict={\n",
    "        'max_depth': 20,\n",
    "        'n_estimators': 100,\n",
    "        'min_samples_leaf': 10,\n",
    "        'random_state': 0\n",
    "    },\n",
    "    prediction_blob:str='predictions/predictions.json',\n",
    "):\n",
    "    ingestion_task = download_taxi_data(bucket_name)\n",
    "    processing_task = preprocess_taxi_data(\n",
    "        input_data=ingestion_task.output,\n",
    "        train_size=train_size,\n",
    "        valid_size=valid_size,\n",
    "        test_size=test_size\n",
    "    )\n",
    "    training_task = train_taxi_data(\n",
    "        train_data=processing_task.outputs[\"train_data\"],\n",
    "        params=params,\n",
    "    )\n",
    "    evaluating_task = evaluate_model(\n",
    "        val_data=processing_task.outputs[\"valid_data\"],\n",
    "        model=training_task.outputs[\"model\"],\n",
    "        target_column_name='duration',\n",
    "        deployment_metric=deployment_metric,\n",
    "        deployment_metric_threshold=deployment_metric_threshold,\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        evaluating_task.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "        # check the container uri list here: https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#scikit-learn\n",
    "        # deploy only if metric value exceeds deployment threshold\n",
    "        registering_task = register_model(\n",
    "            serving_container_uri=serving_container_uri,\n",
    "            project_id=project_id,\n",
    "            region=region,\n",
    "            model_name=model_name,\n",
    "            model=training_task.outputs[\"model\"],\n",
    "        )\n",
    "        \n",
    "        deploying_task = deploy_model(\n",
    "            model_resource_name = registering_task.outputs[\"model_resource_name\"],\n",
    "            project_id=project_id,\n",
    "            region=region\n",
    "        )\n",
    "        \n",
    "        predicting_task = test_prediction(\n",
    "            project_id=project_id,\n",
    "            region=region,\n",
    "            test_ds=processing_task.outputs[\"test_data\"],\n",
    "            endpoint_resource_name=deploying_task.outputs[\"endpoint_resource_name\"],\n",
    "            bucket_name=bucket_name,\n",
    "            prediction_blob=prediction_blob,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded successfully: {'project_id': 'yorrr78-dev-111111', 'region': 'asia-northeast3', 'pipeline_name': 'taxi-data-model-kfp-test', 'pipeline_package_path': 'nytaxi_model_pipeline_job.json', 'bucket_name': 'yorrr78-dev-111111-mlops-bucket', 'train_size': 0.8, 'valid_size': 0.1, 'test_size': 0.1, 'deployment_metric': 'r2', 'deployment_metric_threshold': 0.7, 'serving_container_uri': 'asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest', 'model_name': 'sklearn-kubeflow-nytaxi-regression-model', 'params': {'max_depth': 20, 'n_estimators': 100, 'min_samples_leaf': 10, 'random_state': 0}, 'prediction_blob': 'predictions/predictions.json'}\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=model_pipeline, package_path=\"nytaxi_model_pipeline_job.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'green_tripdata_2022-02.parquet'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=\"nytaxi_model_pipeline_job.json\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
